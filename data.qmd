---
title: Data
description: We describe the sources of our data and the cleaning process.
toc: true
draft: false
---

![](images/data-import-cheatsheet-thumbs.png)

This comes from the file `data.qmd`.

Your first steps in this project will be to find data to work on.

I recommend trying to find data that interests you and that you are knowledgeable about. A bad example would be if you have no interest in video games but your data set is about video games. I also recommend finding data that is related to current events, social justice, and other areas that have an impact.

Initially, you will study *one dataset* but later you will need to combine that data with another dataset. For this reason, I recommend finding data that has some date and/or location components. These types of data are conducive to interesting visualizations and analysis and you can also combine this data with other data that also has a date or location variable. Data from the census, weather data, economic data, are all relatively easy to combine with other data with time/location components.

## What makes a good data set?

-   Data you are interested in and care about.
-   Data where there are a lot of potential questions that you can explore.
-   A data set that isn't completely cleaned already.
-   Multiple sources for data that you can combine.
-   Some type of time and/or location component.

## Where to keep data?

Below 50mb: In `dataset` folder

Above 50mb: In `dataset_ignore` folder. This folder will be ignored by `git` so you'll have to manually sync these files across your team.

### Sharing your data

For small datasets (\<50mb), you can use the `dataset` folder that is tracked by github. Add the files just like you would any other file.

If you create a folder named `data` this will cause problems.

For larger datasets, you'll need to create a new folder in the project root directory named `dataset-ignore`. This will be ignored by git (based off the `.gitignore` file in the project root directory) which will help you avoid issues with Github's size limits. Your team will have to manually make sure the data files in `dataset-ignore` are synced across team members.

Your [load_and_clean_data.R](/scripts/load_and_clean_data.R) file is how you will load and clean your data. Here is a an example of a very simple one.

```{r}
source(
  "scripts/load_and_clean_data.R",
  echo = FALSE # Use echo=FALSE or omit it to avoid code output  
)
```

You should never use absolute paths (eg. `/Users/danielsussman/path/to/project/` or `C:\MA415\\Final_Project\`).

You might consider using the `here` function from the [`here` package](https://here.r-lib.org/articles/here.html) to avoid path problems.

### Load and clean data script

The idea behind this file is that someone coming to your website could largely replicate your analyses after running this script on the original data sets to clean them. This file might create a derivative data set that you then use for your subsequent analysis. Note that you don't need to run this script from every post/page. Instead, you can load in the results of this script, which could be plain text files or `.RData` files. In your data page you'll describe how these results were created. If you have a very large data set, you might save smaller data sets that you can use for exploration purposes. To link to this file, you can use `[cleaning script](/scripts/load_and_clean_data.R)` wich appears as [cleaning script](/scripts/load_and_clean_data.R).

------------------------------------------------------------------------

## Rubric: On this page

You will

-   Describe where/how to find data.
    -   You must include a link to the original data source(s). Make sure to provide attribution to those who collected the data.
    -   Why was the data collected/curated? Who put it together? (This is important, if you don't know why it was collected then that might not be a good dataset to look at.
-   Describe the different data files used and what each variable means.
    -   If you have many variables then only describe the most relevant ones and summarize the rest.
-   Describe any cleaning you had to do for your data.
    -   You *must* include a link to your `load_and_clean_data.R` file.
    -   Rrename variables and recode factors to make data more clear.
    -   Also, describe any additional R packages you used outside of those covered in class.
    -   Describe and show code for how you combined multiple data files and any cleaning that was necessary for that.
    -   Some repetition of what you do in your `load_and_clean_data.R` file is fine and encouraged if it helps explain what you did.
-   Organization, clarity, cleanliness of the page
    -   Make sure to remove excessive warnings, use clean easy-to-read code (without side scrolling), organize with sections, use bullets and other organization tools, etc.
    -   This page should be self-contained.

### Data Background

Our team utilized three main datasets for our analysis:
Country Mapping - ISO, Continent, Region: This dataset was obtained from Kaggle, a platform offering a wide range of free data resources. It was created for mapping countries in the dataset creator's analysis, and she permits its free use. The variables in this dataset are straightforward, naming countries with their ISO codes and categorizing them into different continents, regions, and sub-regions. For instance, Afghanistan is represented as AFG - Asia - Southern Asia.
PISA Scores by Country 2024: Sourced from the World Population Review, this dataset is part of the Programme for International Student Assessment (PISA), coordinated by the Organisation for Economic Co-operation and Development (OECD). PISA assesses the educational performance of 15-year-old students across approximately 80 countries through a two-hour test focused on science, reading, and mathematics. The dataset provides average scores in these subjects, reflecting the educational standards and economic success indicators of the participating countries.
World Inequality Database on Education (WIDE): Available at the World Inequality Database on Education, the WIDE database compiles data from various reliable surveys and assessments, offering insights into the impact of different inequality factors on educational outcomes. Its goal is to highlight significant educational disparities across and within countries to aid in policy-making and public discourse.

The following are the relevant variables for the chosen dataset.
name: Names of the country. 

region: The continent in which the country is located.

sub-region: The specific region of the continent in which the country is located. 

OverallPisaScore2022: Total Pisa scores of the country in 2022.

PISAScoresMathScore2022: The Pisa scores in math of the country in 2022.

PISAScoresScienceScore2022: The Pisa scores in science of the country in 2022.

PISAScoresReadingScore2022: The Pisa scores in reading of the country in 2022.

literacy_1524_no: Percentage of young people aged 15‐24 who can read a simple sentence.

comp_prim_v2_m: Percentage of (i) children and young people aged 3‐5 years above primary school graduation age and (ii) young  people aged 15‐24 years, who have completed primary school. 

mlevel1_m : Passing rate of Level 1 Math Test.

mlevel2_m: Passing rate of Level 2 Math Test.

mlevel3_m: Passing rate of Level 3 Math Test.

eduyears_2024_m: Average number of years of schooling attained for the age group 20–24 years.

eduout_prim_m: Percentage of children of primary school age who are not in school. 

comp_lowsec_v2_m: Percentage of (i) young people aged 3‐5 years above lower secondary school graduation age and (ii) young  people aged 15‐24 years, who have completed lower secondary school.

preschool_3_no: Percentage of 3 to 4 year olds attending any type of pre–primary education programme. 


### Data loading and grouping

[clean_script](/scripts/load_and_clean_data.R%60)

The libraries we use:

```{r}
library(tidyverse)
#A collection of packages for data manipulation and visualization.
library(tidymodels)
#A A suite for modern and tidy statistical modeling.
library(broom) 
#Converts statistical analysis objects into tidy format.
library(scales) 
# For formatting and manipulating axis labels and color scales in plots.
library(patchwork) 
#Facilitates the combination of multiple ggplot2 plots into one.
library(knitr) 
#Enables dynamic report generation integrating R code with text.
library(stringr)
#Provides easy-to-use functions for string operations.
library(car)
# Offers tools for regression diagnostics and other statistical tests.
library(caret)
# Focuses on training and tuning machine learning models.
library(ggplot2)
# Implements a grammar of graphics for creating plots.
library(dplyr)
#Streamlines data manipulation tasks.
library(readxl)
#Allows for reading Excel files into R.
library(MLmetrics)
# Contains functions for computing machine learning metrics.
library(GGally)
#Extends ggplot2 for creating complex visualizations easily.
```

To enhance our data preparation process, we plan to refine the regional categorization within our dataset. Initially, we identified that the 'region' column unexpectedly amalgamated diverse regions, such as combining Europe with North America and East with Southeast Asia. To address this, we will adopt a strategy to split the data more accurately according to geographic regions. We have sourced an additional dataset that delineates regions and continents more distinctly. By integrating this new dataset with our original data, we will reorganize the regional variables for clearer segmentation. Following these adjustments, we will conduct data visualization on the newly grouped data. This step will facilitate a deeper understanding and provide clearer insights by representing the data in a visually intuitive manner. To enhance our data preparation process, we plan to refine the regional categorization within our dataset. Initially, we identified that the 'region' column unexpectedly amalgamated diverse regions, such as combining Europe with North America and East with Southeast Asia. To address this, we will adopt a strategy to split the data more accurately according to geographic regions. We have sourced an additional dataset that delineates regions and continents more distinctly. By integrating this new dataset with our original data, we will reorganize the regional variables for clearer segmentation. Following these adjustments, we will conduct data visualization on the newly grouped data. This step will facilitate a deeper understanding and provide clearer insights by representing the data in a visually intuitive manner.

```{r}
data <- read_csv('dataset/educ.csv')
continent <- read_csv('dataset/continents2.csv')
pisa <- read_csv('dataset/pisa-scores-by-country-2024.csv', show_col_types = FALSE)
```



We try to merge PISA and region dataset, so it can allow us for exploring how educational outcomes vary across different regions, and we can analyse  the influence of regional contexts on educational difference with data.


![](images/clipboard-123080745.png)
