---
title: Data
description: We describe the sources of our data and the cleaning process.
toc: true
draft: false
---

![](images/data-import-cheatsheet-thumbs.png)

This comes from the file `data.qmd`.

Your first steps in this project will be to find data to work on.

I recommend trying to find data that interests you and that you are knowledgeable about. A bad example would be if you have no interest in video games but your data set is about video games. I also recommend finding data that is related to current events, social justice, and other areas that have an impact.

Initially, you will study *one dataset* but later you will need to combine that data with another dataset. For this reason, I recommend finding data that has some date and/or location components. These types of data are conducive to interesting visualizations and analysis and you can also combine this data with other data that also has a date or location variable. Data from the census, weather data, economic data, are all relatively easy to combine with other data with time/location components.

## What makes a good data set?

-   Data you are interested in and care about.
-   Data where there are a lot of potential questions that you can explore.
-   A data set that isn't completely cleaned already.
-   Multiple sources for data that you can combine.
-   Some type of time and/or location component.

## Where to keep data?

Below 50mb: In `dataset` folder

Above 50mb: In `dataset_ignore` folder. This folder will be ignored by `git` so you'll have to manually sync these files across your team.

### Sharing your data

For small datasets (\<50mb), you can use the `dataset` folder that is tracked by github. Add the files just like you would any other file.

If you create a folder named `data` this will cause problems.

For larger datasets, you'll need to create a new folder in the project root directory named `dataset-ignore`. This will be ignored by git (based off the `.gitignore` file in the project root directory) which will help you avoid issues with Github's size limits. Your team will have to manually make sure the data files in `dataset-ignore` are synced across team members.

Your [load_and_clean_data.R](/scripts/load_and_clean_data.R) file is how you will load and clean your data. Here is a an example of a very simple one.

```{r}
source(
  "scripts/load_and_clean_data.R",
  echo = FALSE # Use echo=FALSE or omit it to avoid code output  
)
```

You should never use absolute paths (eg. `/Users/danielsussman/path/to/project/` or `C:\MA415\\Final_Project\`).

You might consider using the `here` function from the [`here` package](https://here.r-lib.org/articles/here.html) to avoid path problems.

### Load and clean data script

The idea behind this file is that someone coming to your website could largely replicate your analyses after running this script on the original data sets to clean them. This file might create a derivative data set that you then use for your subsequent analysis. Note that you don't need to run this script from every post/page. Instead, you can load in the results of this script, which could be plain text files or `.RData` files. In your data page you'll describe how these results were created. If you have a very large data set, you might save smaller data sets that you can use for exploration purposes. To link to this file, you can use `[cleaning script](/scripts/load_and_clean_data.R)` wich appears as [cleaning script](/scripts/load_and_clean_data.R).

------------------------------------------------------------------------

## Rubric: On this page

You will

-   Describe where/how to find data.
    -   You must include a link to the original data source(s). Make sure to provide attribution to those who collected the data.
    -   Why was the data collected/curated? Who put it together? (This is important, if you don't know why it was collected then that might not be a good dataset to look at.
-   Describe the different data files used and what each variable means.
    -   If you have many variables then only describe the most relevant ones and summarize the rest.
-   Describe any cleaning you had to do for your data.
    -   You *must* include a link to your `load_and_clean_data.R` file.
    -   Rrename variables and recode factors to make data more clear.
    -   Also, describe any additional R packages you used outside of those covered in class.
    -   Describe and show code for how you combined multiple data files and any cleaning that was necessary for that.
    -   Some repetition of what you do in your `load_and_clean_data.R` file is fine and encouraged if it helps explain what you did.
-   Organization, clarity, cleanliness of the page
    -   Make sure to remove excessive warnings, use clean easy-to-read code (without side scrolling), organize with sections, use bullets and other organization tools, etc.
    -   This page should be self-contained.

### Data Background

Original Data Source: The dataset is available at the World Inequality Database on Education (WIDE). [World Inequality Database on Education](World Inequality Database on Education) Wefound a data set that is related to education vs race. Then we found that the data set can track educational disparities across various countries, including the United States. Specifically, the data originates from Demographic and Health Surveys (DHS), Multiple Indicator Cluster Surveys (MICS), and other national household surveys and learning assessments across over 170 countries. The World Inequality Database on Education (WIDE) serves as a rich and free repository of data sourced from a variety of reliable surveys and assessments, providing insights into how educational outcomes are influenced by different factors of inequality. It hopes to draw attention to unacceptable levels of education inequality across countries and between groups within countries, to help to inform policy design and public debate.

We used two datasets so far. We mainly focused on the education dataset which includes many education-related variables that are used to assess various aspects of educational attainment and attendance rates across different levels of schooling, from pre-primary to upper-secondary education. Variables and their definitions: Pre‐primary education attendance: Percentage of 3 to 4 year olds attending any type of pre–primary education programme. Never been to school: Percentage of children aged 3‐6 years above primary school entrance age who have never been to school. Over‐age primary school attendance: Percentage of children in primary school who are at least two years older than the official age for grade. Out‐of‐school children: Percentage of children of primary school age who are not in school. Primary completion rate: Percentage of (i) children and young people aged 3‐5 years above primary school graduation age and (ii) young people aged 15‐24 years, who have completed primary school. Transition rate to lower secondary school: Number of young people attending the first grade of lower secondary school as a percentage of those attending the final grade of primary school. Out‐of‐school adolescents: Percentage of adolescents of lower secondary school age who are not in school. Lower secondary completion rate: Percentage of (i) young people aged 3‐5 years above lower secondary school graduation age and (ii) young people aged 15‐24 years, who have completed lower secondary school. Transition rate to upper secondary school: Number of young people attending the first grade of upper secondary school as a percentage of those attending the final grade of lower secondary school. Out‐of‐school youth: Percentage of youth of upper secondary school age who are not in school. Upper secondary completion rate: Percentage of (i) young people aged 3‐5 years above upper secondary school graduation age and (ii) people aged 20‐29 years, who have completed upper secondary school. Tertiary completion rate: Percentage of people aged 25‐29 years, who have completed at least four years of higher education. Less than 2 years of schooling: Percentage of the population living in extreme education poverty: with less than two years of education for the age group 20‐24 years. Less than 4 years of schooling: Percentage of the population living in education poverty: with less than four years of education for the age group 20‐24 years. Mean years of education: Average number of years of schooling attained for the age group 20–24 years.

Our data is mainly focused on education access and completion conditions by categorizing different demographics such as sex, ethnicity, location, region, wealth, and speaking language at home. The data also combined those categories to narrow down the group and get the education data. For example, we had the category of “Location & Sex,” which groups data with either Rural or Urban, each further grouped by either female or male. Thus, we have four different groups of data: Rural-Female, Rural-Male, Urban-Female, and Urban-Male. However, to avoid overfitting the model or processing duplicate data, we may filter the data by a single category of either one of the “Location”, ”Sex”, “Ethnicity”, ”Region”, and “Wealth,” discarding other combined ones. For response value, we will keep those variables about the access and completion rate and remove those indicators about learning indicators such as learning achievement in reading/mathematics/science because there are a lot of missing values.

For future analysis of education based on race, we also used another dataset that contains the name of the country, the country code, the central region (e.g. Asia), and its sub-region (e.g. Southern Asia) in order to regroup the area in the education dataset. the area in the education data set.

### Data loading and grouping

[clean_script](/scripts/load_and_clean_data.R%60)

The libraries we use:

```{r}
library(tidyverse)
library(tidymodels)
library(broom)
```

To enhance our data preparation process, we plan to refine the regional categorization within our dataset. Initially, we identified that the 'region' column unexpectedly amalgamated diverse regions, such as combining Europe with North America and East with Southeast Asia. To address this, we will adopt a strategy to split the data more accurately according to geographic regions. We have sourced an additional dataset that delineates regions and continents more distinctly. By integrating this new dataset with our original data, we will reorganize the regional variables for clearer segmentation. Following these adjustments, we will conduct data visualization on the newly grouped data. This step will facilitate a deeper understanding and provide clearer insights by representing the data in a visually intuitive manner. To enhance our data preparation process, we plan to refine the regional categorization within our dataset. Initially, we identified that the 'region' column unexpectedly amalgamated diverse regions, such as combining Europe with North America and East with Southeast Asia. To address this, we will adopt a strategy to split the data more accurately according to geographic regions. We have sourced an additional dataset that delineates regions and continents more distinctly. By integrating this new dataset with our original data, we will reorganize the regional variables for clearer segmentation. Following these adjustments, we will conduct data visualization on the newly grouped data. This step will facilitate a deeper understanding and provide clearer insights by representing the data in a visually intuitive manner.

```{r}
data <- read_csv('dataset/educ.csv')
continent <- read_csv('dataset/continents2.csv')
```

```{r}
continent <- continent|>
  rename(
    country = name
  )
continent

merged_data <- merge(data, continent, by = "country")

merged_data <- merged_data |> rename(sub_region = `sub-region`)
```

![](images/clipboard-123080745.png)
